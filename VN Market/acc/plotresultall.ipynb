{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb9q2_QZgdNk"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/AI4Finance-Foundation/FinRL-Tutorials/blob/master/2-Advance/FinRL_Ensemble_StockTrading_ICAIF_2020.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mPT0ipYE28wL",
        "outputId": "2cbfe5e0-e6dd-42b6-cad3-2da680e64216"
      },
      "outputs": [],
      "source": [
        "# # ## install finrl library\n",
        "# !pip install wrds\n",
        "# !pip install swig\n",
        "# # !pip install -q condacolab\n",
        "# # import condacolab\n",
        "# # condacolab.install()\n",
        "# !apt-get update -y -qq && apt-get install -y -qq cmake libopenmpi-dev python3-dev zlib1g-dev libgl1-mesa-glx swig\n",
        "# !pip install git+https://github.com/AI4Finance-Foundation/FinRL.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWp1PEEMgaL_",
        "outputId": "fb61f0a3-e6be-4e0a-f9d4-e08ddcb959f0"
      },
      "outputs": [],
      "source": [
        "# !pip install yfinance pandas numpy matplotlib stockstats gym stable-baselines3 tensorflow pyfolio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gJrDsohglGA",
        "outputId": "c2ffa2ed-b722-4ace-a3e8-0d24a0b17fdd"
      },
      "outputs": [],
      "source": [
        "# !pip install pandas_market_calendars"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGv01K8Sh1hn"
      },
      "source": [
        "<a id='1.3'></a>\n",
        "## 2.3. Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EeMK7Uentj1V"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr2zX7ZxNyFQ"
      },
      "source": [
        "<a id='6.1'></a>\n",
        "## 7.1 BackTestStats\n",
        "pass in df_account_value, this information is stored in env class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "# matplotlib.use('Agg')\n",
        "import datetime\n",
        "import statistics \n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "\n",
        "%matplotlib inline\n",
        "from FinRL.finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "from FinRL.finrl.meta.preprocessor.preprocessors import FeatureEngineer, data_split\n",
        "from FinRL.finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
        "from FinRL.finrl.agents.stablebaselines3.models import DRLAgent,DRLEnsembleAgent\n",
        "from FinRL.finrl.plot import backtest_stats, backtest_plot, get_daily_return, get_baseline\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from pprint import pprint\n",
        "\n",
        "import sys\n",
        "sys.path.append(\"../FinRL-Library\")\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_account_value = pd.read_csv('df_account_value.csv', index_col=0)\n",
        "df_account_value_a2c = pd.read_csv('df_account_value_a2c.csv', index_col=0)\n",
        "df_account_value_ppo = pd.read_csv('df_account_value_ppo.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzkr9yv-AdV_",
        "outputId": "ab0971b8-10b0-4fb1-a151-71a1de89cdf2",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==============Get Backtest Results===========\n",
            "Annual return          0.005958\n",
            "Cumulative returns     0.025567\n",
            "Annual volatility      0.101266\n",
            "Sharpe ratio           0.109669\n",
            "Calmar ratio           0.026887\n",
            "Stability              0.694287\n",
            "Max drawdown          -0.221592\n",
            "Omega ratio            1.022288\n",
            "Sortino ratio          0.140565\n",
            "Skew                        NaN\n",
            "Kurtosis                    NaN\n",
            "Tail ratio             1.086881\n",
            "Daily value at risk   -0.012714\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(\"==============Get Backtest Results===========\")\n",
        "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
        "\n",
        "perf_stats_all = backtest_stats(account_value=df_account_value)\n",
        "perf_stats_all = pd.DataFrame(perf_stats_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==============Get Backtest Results===========\n",
            "Annual return          0.081611\n",
            "Cumulative returns     0.405763\n",
            "Annual volatility      0.151941\n",
            "Sharpe ratio           0.593186\n",
            "Calmar ratio           0.374945\n",
            "Stability              0.720479\n",
            "Max drawdown          -0.217661\n",
            "Omega ratio            1.120370\n",
            "Sortino ratio          0.805448\n",
            "Skew                        NaN\n",
            "Kurtosis                    NaN\n",
            "Tail ratio             0.979209\n",
            "Daily value at risk   -0.018785\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "print(\"==============Get Backtest Results===========\")\n",
        "now = datetime.datetime.now().strftime('%Y%m%d-%Hh%M')\n",
        "\n",
        "perf_stats_all = backtest_stats(account_value=df_account_value_ppo)\n",
        "perf_stats_all = pd.DataFrame(perf_stats_all)\n",
        "# ==============Get Backtest Results===========\n",
        "# Annual return               0.379725\n",
        "# Cumulative returns          0.083797\n",
        "# Annual volatility      338567.947564\n",
        "# Sharpe ratio                2.017211\n",
        "# Calmar ratio                0.379725\n",
        "# Stability                   0.324152\n",
        "# Max drawdown               -1.000000\n",
        "# Omega ratio             22751.286315\n",
        "# Sortino ratio          155782.345252\n",
        "# Skew                             NaN\n",
        "# Kurtosis                         NaN\n",
        "# Tail ratio                  9.187028\n",
        "# Daily value at risk    -39945.381418"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DiHhM1YkoCel",
        "outputId": "c233f613-67a3-4882-8710-c1839247590e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==============Get Baseline Stats===========\n",
            "Shape of DataFrame:  (1070, 8)\n",
            "Annual return          0.044268\n",
            "Cumulative returns     0.201921\n",
            "Annual volatility      0.194809\n",
            "Sharpe ratio           0.320864\n",
            "Calmar ratio           0.119365\n",
            "Stability              0.838783\n",
            "Max drawdown          -0.370862\n",
            "Omega ratio            1.076707\n",
            "Sortino ratio          0.428540\n",
            "Skew                        NaN\n",
            "Kurtosis                    NaN\n",
            "Tail ratio             0.874737\n",
            "Daily value at risk   -0.024296\n",
            "dtype: float64\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#baseline stats\n",
        "print(\"==============Get Baseline Stats===========\")\n",
        "df_dji_ = get_baseline(\n",
        "        ticker=\"^DJI\",\n",
        "        start = df_account_value.loc[0,'date'],\n",
        "        end = df_account_value.loc[len(df_account_value)-1,'date'])\n",
        "\n",
        "stats = backtest_stats(df_dji_, value_col_name = 'close')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhJ9whD75WTs",
        "outputId": "8ae25787-8400-4357-ecc0-af7538689cee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_dji:              date           dji\n",
            "0     2015-12-31  1.000000e+06\n",
            "1     2016-01-04  9.841556e+05\n",
            "2     2016-01-05  9.847134e+05\n",
            "3     2016-01-06  9.702428e+05\n",
            "4     2016-01-07  9.477229e+05\n",
            "...          ...           ...\n",
            "1066  2020-03-27  1.241707e+06\n",
            "1067  2020-03-30  1.281345e+06\n",
            "1068  2020-03-31  1.257798e+06\n",
            "1069  2020-04-01  1.201921e+06\n",
            "1070  2020-04-02           NaN\n",
            "\n",
            "[1071 rows x 2 columns]\n",
            "df_dji:                       dji\n",
            "date                    \n",
            "2015-12-31  1.000000e+06\n",
            "2016-01-04  9.841556e+05\n",
            "2016-01-05  9.847134e+05\n",
            "2016-01-06  9.702428e+05\n",
            "2016-01-07  9.477229e+05\n",
            "...                  ...\n",
            "2020-03-27  1.241707e+06\n",
            "2020-03-30  1.281345e+06\n",
            "2020-03-31  1.257798e+06\n",
            "2020-04-01  1.201921e+06\n",
            "2020-04-02           NaN\n",
            "\n",
            "[1071 rows x 1 columns]\n"
          ]
        }
      ],
      "source": [
        "df_dji = pd.DataFrame()\n",
        "df_dji['date'] = df_account_value['date']\n",
        "df_dji['dji'] = df_dji_['close'] / df_dji_['close'][0] * 1000000\n",
        "print(\"df_dji: \", df_dji)\n",
        "df_dji.to_csv(\"df_dji.csv\")\n",
        "df_dji = df_dji.set_index(df_dji.columns[0])\n",
        "print(\"df_dji: \", df_dji)\n",
        "df_dji.to_csv(\"df_dji+.csv\")\n",
        "\n",
        "df_account_value.to_csv('df_account_value.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9U6Suru3h1jc"
      },
      "source": [
        "<a id='6.2'></a>\n",
        "## 7.2 BackTestPlot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StockData tickers: ['AAPL', 'AXP', 'BA', 'CAT', 'CSCO', 'CVX', 'DD', 'DIS', 'GE', 'GS', 'HD', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PFE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WMT', 'XOM']\n",
            "Number of tickers: 29\n",
            "arReturns shape: (1696, 29)\n",
            "covReturns shape: (29, 29)\n",
            "Cleaned weights keys: ['AAPL', 'AXP', 'BA', 'CAT', 'CSCO', 'CVX', 'DD', 'DIS', 'GE', 'GS', 'HD', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM', 'MRK', 'MSFT', 'NKE', 'PFE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WMT', 'XOM']\n",
            "Number of weights: 29\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cvxpy as cp\n",
        "processed = pd.read_csv('processed.csv', index_col=0)\n",
        "stock_dimension = len(processed.tic.unique())\n",
        "df_result_ensemble = pd.DataFrame({'date': df_account_value['date'], \n",
        "                                 'ensemble': df_account_value['account_value']}).set_index('date')\n",
        "\n",
        "df_result_a2c = pd.DataFrame({'date': df_account_value_a2c['date'], \n",
        "                             'a2c': df_account_value_a2c['account_value']}).set_index('date')\n",
        "\n",
        "df_result_ppo = pd.DataFrame({'date': df_account_value_ppo['date'], \n",
        "                             'ppo': df_account_value_ppo['account_value']}).set_index('date')\n",
        "\n",
        "# Merge all DataFrames using their date index\n",
        "result = df_result_ensemble.join([df_result_ppo, df_result_a2c, df_dji])\n",
        "\n",
        "TRAIN_START_DATE = '2009-01-01'\n",
        "TRAIN_END_DATE = '2015-09-30'\n",
        "TEST_START_DATE = '2015-09-30'\n",
        "TEST_END_DATE = '2020-05-08'\n",
        "# Helper function to process dataframe for portfolio optimization\n",
        "def process_df_for_mvo(df):\n",
        "    df = df.sort_values(['date', 'tic'], ignore_index=True)[['date', 'tic', 'close']]\n",
        "    fst = df.iloc[0:stock_dimension, :]\n",
        "    tic = fst['tic'].tolist()\n",
        "\n",
        "    mvo = pd.DataFrame()\n",
        "    for k in range(len(tic)):\n",
        "        mvo[tic[k]] = 0\n",
        "\n",
        "    for i in range(df.shape[0] // stock_dimension):\n",
        "        n = df.iloc[i * stock_dimension : (i + 1) * stock_dimension, :]\n",
        "        date = n['date'].iloc[0]  # Use .iloc to ensure single date\n",
        "        mvo.loc[date] = n['close'].tolist()\n",
        "\n",
        "    return mvo\n",
        "\n",
        "# Helper function to compute stock returns (in decimal form)\n",
        "def StockReturnsComputing(StockPrice, Rows, Columns):\n",
        "    StockReturn = np.zeros([Rows - 1, Columns])\n",
        "    for j in range(Columns):\n",
        "        for i in range(Rows - 1):\n",
        "            StockReturn[i, j] = (StockPrice[i + 1, j] - StockPrice[i, j]) / StockPrice[i, j]\n",
        "    return StockReturn\n",
        "\n",
        "# Assuming data_split, processed_full, TRAIN_START_DATE, TRAIN_END_DATE, TEST_START_DATE, TEST_END_DATE, and stock_dimension are defined\n",
        "train_mvo = data_split(processed, TRAIN_START_DATE, TRAIN_END_DATE).reset_index()\n",
        "trade_mvo = data_split(processed, TEST_START_DATE, TEST_END_DATE).reset_index()\n",
        "\n",
        "# Process data for training and trading periods\n",
        "StockData = process_df_for_mvo(train_mvo)\n",
        "TradeData = process_df_for_mvo(trade_mvo)\n",
        "\n",
        "# Get tickers\n",
        "tickers = StockData.columns.tolist()\n",
        "print(\"StockData tickers:\", tickers)\n",
        "print(\"Number of tickers:\", len(tickers))\n",
        "\n",
        "# Compute asset returns\n",
        "arStockPrices = np.asarray(StockData)\n",
        "Rows, Cols = arStockPrices.shape\n",
        "arReturns = StockReturnsComputing(arStockPrices, Rows, Cols)\n",
        "print(\"arReturns shape:\", arReturns.shape)\n",
        "\n",
        "# Compute mean returns and covariance matrix\n",
        "meanReturns = np.mean(arReturns, axis=0)\n",
        "covReturns = np.cov(arReturns, rowvar=False)\n",
        "print(\"covReturns shape:\", covReturns.shape)\n",
        "\n",
        "# Check for NaN in covariance matrix\n",
        "if np.any(np.isnan(covReturns)):\n",
        "    print(\"Warning: covReturns contains NaN values. Check input data for missing or invalid prices.\")\n",
        "    raise ValueError(\"NaN values in covariance matrix\")\n",
        "\n",
        "# Convert meanReturns and covReturns to pandas with tickers\n",
        "meanReturns = pd.Series(meanReturns, index=tickers)\n",
        "covReturns = pd.DataFrame(covReturns, index=tickers, columns=tickers)\n",
        "\n",
        "# ── Replace pypfopt EfficientFrontier with CVXPY-based Min Variance ─────────────\n",
        "# 1) Extract Σ (covariance matrix) in the correct ticker order\n",
        "Σ = covReturns.loc[tickers, tickers].to_numpy()  # shape (n, n)\n",
        "n = len(tickers)\n",
        "\n",
        "# 2) Define CVXPY variable\n",
        "w = cp.Variable(n)\n",
        "\n",
        "# 3) Define objective: minimize w^T Σ w\n",
        "objective = cp.Minimize(cp.quad_form(w, Σ))\n",
        "\n",
        "# 4) Define constraints: sum(w) == 1, 0 <= w_i <= 0.5\n",
        "constraints = [\n",
        "    cp.sum(w) == 1,\n",
        "    w >= 0,\n",
        "    w <= 0.5\n",
        "]\n",
        "\n",
        "# 5) Formulate and solve the problem\n",
        "prob = cp.Problem(objective, constraints)\n",
        "prob.solve(solver=cp.SCS)\n",
        "\n",
        "# 6) Check solver status\n",
        "if prob.status not in [\"optimal\", \"optimal_inaccurate\"]:\n",
        "    raise ValueError(f\"CVXPY did not find an optimal solution (status={prob.status})\")\n",
        "\n",
        "# 7) Extract raw weights and clean them (clamp negatives to zero, re-normalize)\n",
        "raw_weights = w.value\n",
        "cleaned = np.maximum(raw_weights, 0)\n",
        "cleaned = cleaned / cleaned.sum()\n",
        "\n",
        "# 8) Map cleaned weights back into a dict keyed by ticker\n",
        "cleaned_weights_min_var = {tickers[i]: float(cleaned[i]) for i in range(n)}\n",
        "\n",
        "# Debug: Check weights\n",
        "print(\"Cleaned weights keys:\", list(cleaned_weights_min_var.keys()))\n",
        "print(\"Number of weights:\", len(cleaned_weights_min_var))\n",
        "\n",
        "# Verify ticker alignment\n",
        "missing_tickers = [tic for tic in tickers if tic not in cleaned_weights_min_var]\n",
        "if missing_tickers:\n",
        "    print(\"Error: Missing tickers in weights:\", missing_tickers)\n",
        "    raise KeyError(f\"Tickers {missing_tickers} not found in cleaned_weights_min_var\")\n",
        "\n",
        "# Compute dollar allocations ($1,000,000 total portfolio value)\n",
        "min_var_weights = np.array([1_000_000 * cleaned_weights_min_var[tic] for tic in tickers])\n",
        "\n",
        "# Compute number of shares based on last training period prices\n",
        "# (assuming fractional shares are allowed for backtesting)\n",
        "LastPrice = np.array([1 / p for p in StockData.tail(1).to_numpy()[0]])\n",
        "Initial_Portfolio_min_var = np.multiply(min_var_weights, LastPrice)\n",
        "\n",
        "# Calculate portfolio value over the trade period\n",
        "# Ensure TradeData columns align with `tickers` order\n",
        "TradeData = TradeData[tickers]\n",
        "Portfolio_Assets_min_var = TradeData @ Initial_Portfolio_min_var\n",
        "\n",
        "# Create DataFrame for Min Variance results\n",
        "df_account_value_min_var = pd.DataFrame({\n",
        "    'date': TradeData.index,\n",
        "    'account_value': Portfolio_Assets_min_var\n",
        "}).reset_index(drop=True)\n",
        "\n",
        "# Prepare result DataFrame for comparison\n",
        "df_result_min_var = pd.DataFrame({\n",
        "    'date': df_account_value_min_var['date'],\n",
        "    'min_var': df_account_value_min_var['account_value']\n",
        "}).set_index('date')\n",
        "\n",
        "# Assuming df_result_ensemble, df_result_ppo, df_result_a2c, and df_dji are defined\n",
        "# Merge all strategy results\n",
        "result = df_result_ensemble.join([df_result_ppo, df_result_a2c, df_result_min_var, df_dji], how='inner')\n",
        "\n",
        "# Calculate cumulative returns\n",
        "initial_ensemble = result['ensemble'].iloc[0]\n",
        "initial_ppo = result['ppo'].iloc[0]\n",
        "initial_a2c = result['a2c'].iloc[0]\n",
        "initial_min_var = result['min_var'].iloc[0]\n",
        "initial_dji = result['dji'].iloc[0]\n",
        "\n",
        "df_cumulative = pd.DataFrame({\n",
        "    'Ensemble': (result['ensemble'] / initial_ensemble - 1) * 100,\n",
        "    'PPO':      (result['ppo']      / initial_ppo      - 1) * 100,\n",
        "    'A2C':      (result['a2c']      / initial_a2c      - 1) * 100,\n",
        "    'Min Var':  (result['min_var']  / initial_min_var  - 1) * 100,\n",
        "    'DJI':      (result['dji']      / initial_dji      - 1) * 100\n",
        "}, index=result.index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# … all your data-prep up through df_cumulative …\n",
        "# ensure the index is parsed as real timestamps\n",
        "df_cumulative.index = pd.to_datetime(df_cumulative.index)\n",
        "\n",
        "# Create a figure and axes explicitly\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "\n",
        "# Plot the cumulative returns (this draws your four lines exactly as before)\n",
        "df_cumulative.plot(ax=ax)\n",
        "\n",
        "# —— now override the default x-axis ticks & grid —— \n",
        "\n",
        "# 1. Major ticks every quarter (Jan, Apr, Jul, Oct)\n",
        "ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=[1,4,7,10]))\n",
        "\n",
        "# 2. Format each tick as “Jan 4” on line 1, “2016” on line 2\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d\\n%Y'))\n",
        "\n",
        "# 3. Draw vertical gridlines at those quarter-points, plus horizontal at y-majors\n",
        "ax.grid(axis='x', which='major', linestyle='--', linewidth=0.5)\n",
        "ax.grid(axis='y', which='major', linestyle='--', linewidth=0.5)\n",
        "\n",
        "# 4. Tweak tick label alignment\n",
        "plt.setp(ax.get_xticklabels(), rotation=0, ha='center')\n",
        "\n",
        "# —— finish off your labels/legend/title exactly as before —— \n",
        "\n",
        "ax.set_title('Cumulative Return Comparison of Trading Strategies vith Transaction Cost')\n",
        "ax.set_xlabel('Date')\n",
        "ax.set_ylabel('Cumulative Return (%)')\n",
        "ax.legend(['Ensemble', 'PPO', 'A2C', 'DJIA', 'Min var'])\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            turbulence\n",
            "date                  \n",
            "2015-07-30   61.872816\n",
            "2018-04-12   28.966606\n",
            "2009-12-01    0.000000\n",
            "2017-03-01   32.012036\n",
            "2009-03-27    0.000000\n",
            "2018-12-27   18.099139\n",
            "2018-12-03   43.325629\n",
            "2012-09-28   23.538185\n",
            "2016-02-09   46.182045\n",
            "2020-02-18   27.705211\n",
            "2019-08-22   21.326631\n",
            "2018-09-07   20.580350\n",
            "2014-10-29   15.992378\n",
            "2013-11-01   40.870889\n",
            "2009-02-06    0.000000\n",
            "2009-02-24    0.000000\n",
            "2016-03-21   26.296663\n",
            "2019-02-15   26.356619\n",
            "2010-07-02   13.821799\n",
            "2017-09-27   45.143934\n"
          ]
        }
      ],
      "source": [
        "# assuming `processed` has columns ['date','tic',...,'turbulence']\n",
        "turb_per_date = (\n",
        "    processed[['date','turbulence']]\n",
        "    .drop_duplicates('date')\n",
        "    .set_index('date')\n",
        "    .sort_index()\n",
        ")\n",
        "\n",
        "# now turb_per_date is one row per date:\n",
        "print(turb_per_date.sample(20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# … your data prep up through df_cumulative …\n",
        "\n",
        "start, end = \"2020-01-02\", \"2020-03-30\"\n",
        "df_win = df_cumulative.loc[start:end]\n",
        "\n",
        "# Make turbulence a Series, not DataFrame\n",
        "turb_win = turb_per_date['turbulence'].loc[start:end]\n",
        "turb_win.index = pd.to_datetime(turb_win.index)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(15, 5))\n",
        "df_win.plot(ax=ax, linewidth=1.5)\n",
        "ax.set_xlim(start, end)\n",
        "\n",
        "ax.xaxis.set_major_locator(mdates.DayLocator(interval=9))\n",
        "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b %d'))\n",
        "ax.grid(axis='x', which='major', linestyle='--', linewidth=0.5)\n",
        "ax.grid(axis='y', which='major', linestyle='--', linewidth=0.5)\n",
        "\n",
        "ax.set_ylabel('Cumulative Return (%)')\n",
        "ax.legend(['Ensemble', 'PPO', 'A2C', 'DJIA', 'Min var'])\n",
        "\n",
        "# Twin axis — now turb_win is a Series\n",
        "ax2 = ax.twinx()\n",
        "ax2.plot(turb_win.index, turb_win.values,\n",
        "         color='red', linestyle='--', linewidth=2,\n",
        "         label='Turbulence Index')\n",
        "ax2.set_ylabel('Turbulence Index')\n",
        "\n",
        "# This will now be a single float\n",
        "ax2.set_ylim(0, turb_win.max() * 1.1)\n",
        "\n",
        "# Merge legends\n",
        "lines, labs = ax.get_legend_handles_labels()\n",
        "l2, la2 = ax2.get_legend_handles_labels()\n",
        "ax.legend(lines + l2, labs + la2, loc='lower left', frameon=False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBQx4bVQFi-a"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv_finrl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
